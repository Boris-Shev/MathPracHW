{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ac9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90be0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b467d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\",\n",
    "                             analyzer='word', binary=True, max_df = 100, min_df = 3)\n",
    "vectorizer.fit(newsgroups_train.data)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7442f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = [dict(zip(arr[i] ,vectorizer.inverse_transform([X[i]])[0])) for i in tqdm(range(ndocs))]\n",
    "def LDA(X, a, b, ndocs, nwords, ntags, niter):\n",
    "    n_kw = np.zeros((nwords, ntags))\n",
    "    n_dk = np.zeros((ndocs, ntags))\n",
    "    n_k = np.zeros(ntags)\n",
    "    p_k = np.zeros(ntags)\n",
    "    arr = [np.nonzero(X[i])[0] for i in range(ndocs)]\n",
    "    length = [len(arr[i]) for i in range(ndocs)] \n",
    "    \n",
    "    for d in range(ndocs):\n",
    "        for w in range(length[d]):\n",
    "            tmp = int(np.floor(ntags * np.random.rand()))\n",
    "            X[d, arr[d][w]] = tmp + 1 # Аккуратно: здесь хранится тег слова + 1\n",
    "            n_k[tmp] += 1\n",
    "            n_kw[arr[d][w], tmp] += 1\n",
    "            n_dk[d, tmp] += 1\n",
    "    \n",
    "    \n",
    "#     for _ in tqdm(range(niter)):\n",
    "#         for d in tqdm(range(ndocs)):\n",
    "#             n_k[X[d, arr[d]] - 1] -= 1\n",
    "#             n_kw[arr[d], X[d, arr[d]] - 1] -= 1\n",
    "#             n_dk[d, X[d, arr[d]] - 1] -= 1\n",
    "            \n",
    "#             prob = [((n_dk[d, i] + a) * (n_kw[arr[d], i] + b) /\n",
    "#                 (n_k[i] + nwords)) for i in range(ntags)]\n",
    "#             prob = np.transpose(prob)\n",
    "           \n",
    "#             X[d, arr[d]] = np.array([int(np.random.choice(\n",
    "#                 range(ntags), 1, p = prob[i] / prob[i].sum()))\n",
    "#                 for i in range(length[d])]) + 1\n",
    "            \n",
    "#             n_k[X[d, arr[d]] - 1] += 1\n",
    "#             n_kw[arr[d], X[d, arr[d]] - 1] += 1\n",
    "#             n_dk[d, X[d, arr[d]] - 1] += 1\n",
    "            \n",
    "    \n",
    "    for _ in tqdm(range(niter)):\n",
    "        for d in range(ndocs):\n",
    "            for w in range(length[d]):\n",
    "                n_k[X[d, arr[d][w]] - 1] -= 1\n",
    "                n_kw[arr[d][w], X[d, arr[d][w]] - 1] -= 1\n",
    "                n_dk[d, X[d, arr[d][w]] - 1] -= 1\n",
    "                \n",
    "                for k in range(ntags):\n",
    "                    p_k[k] = (n_dk[d, k] + a) * (n_kw[arr[d][w], k] + b) / (n_k[k] + nwords)\n",
    "                X[d, arr[d][w]] = np.random.choice(range(ntags), 1, p = p_k / p_k.sum()) + 1\n",
    "                \n",
    "                n_k[X[d, arr[d][w]] - 1] += 1\n",
    "                n_kw[arr[d][w], X[d, arr[d][w]] - 1] += 1\n",
    "                n_dk[d, X[d, arr[d][w]] - 1] += 1\n",
    "            \n",
    "    return n_kw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3011f208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3ab7dc415d4907b9926b969e858c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  armenian  armenians  turkish  arab  armed  soldiers  argic  innocent  serdar  criminal\n",
      "\n",
      "2 :  launch  escrow  enforcement  privacy  encrypted  agencies  industry  orbit  providing  shuttle\n",
      "\n",
      "3 :  rangers  nhl  cup  fans  pittsburgh  playoffs  detroit  1st  ice  montreal\n",
      "\n",
      "4 :  cpu  256  microsoft  upgrade  setup  default  x11r5  meg  host  client\n",
      "\n",
      "5 :  curious  damn  baud  math  simon  suggestion  kent  stuck  atf  imho\n",
      "\n",
      "6 :  poster  odd  flow  versa  handling  shut  controls  solve  configuration  handles\n",
      "\n",
      "7 :  blow  sp  suggested  pa  brother  charles  spots  favorite  humor  differences\n",
      "\n",
      "8 :  walk  postings  curious  yesterday  powerbook  anymore  club  occur  tuned  controlled\n",
      "\n",
      "9 :  sin  heaven  holy  interpretation  spirit  catholic  biblical  scripture  eternal  religions\n",
      "\n",
      "10 :  sales  waste  packaging  kent  experiences  sleep  repost  bet  electronics  technologies\n",
      "\n",
      "11 :  amazing  ah  thanx  checked  opposed  till  cool  indicates  gonna  naturally\n",
      "\n",
      "12 :  smaller  enjoy  walk  ya  twice  drops  camera  bush  dx  slowly\n",
      "\n",
      "13 :  vice  sea  tek  manhattan  ico  sank  bronx  blew  beauchaine  queens\n",
      "\n",
      "14 :  helpful  taste  poster  paying  reader  equal  em  spare  specs  sigh\n",
      "\n",
      "15 :  surrender  pitt  skepticism  disease  geb  intellect  chastity  cadre  n3jxp  dsl\n",
      "\n",
      "16 :  ride  dod  riding  wheel  rear  battery  oil  auto  bikes  motorcycle\n",
      "\n",
      "17 :  ah  mo  ma  mq  mi  ml  wm  mn  sp  mw\n",
      "\n",
      "18 :  alot  gold  insurance  chances  economy  shut  programmer  incorrect  jon  combination\n",
      "\n",
      "19 :  ridiculous  wonderful  expansion  imho  curious  semi  comparison  super  anymore  thanx\n",
      "\n",
      "20 :  uucp  corp  networking  uunet  205  intergraph  730  837  b30  ssd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a, b = 1, 1\n",
    "nnwords = X_train.shape[1]\n",
    "nndocs = X_train.shape[0]\n",
    "nntags = 20\n",
    "nniter = 80\n",
    "XX = X_train.toarray()\n",
    "res = np.transpose(LDA(XX, a, b, nndocs, nnwords, nntags, nniter))\n",
    "dictionary = vectorizer.inverse_transform([np.ones(nnwords)])[0]\n",
    "for i in range(nntags):\n",
    "    print(i + 1, \": \", \"  \".join(dictionary[np.argsort(res[i])[-1:-11:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbcfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
